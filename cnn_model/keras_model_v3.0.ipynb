{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Package Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The backend is: tensorflow\n",
      "tf\n",
      "1.11.0\n",
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "\n",
    "# clear warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# import keras data\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf') # note that we need to have tensorflow dimension ordering still because of the weigths.\n",
    "print('The backend is:',K.backend())\n",
    "import tensorflow as tf\n",
    "print(K.image_dim_ordering()) # should say tf\n",
    "print(tf.__version__) # tested for 1.11.0\n",
    "import cv2\n",
    "import keras\n",
    "print(keras.__version__) # tested for 2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "from __future__ import absolute_import, division, print_function # make it compatible w Python 2\n",
    "import os\n",
    "import h5py # to handle weights\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# to read image\n",
    "from PIL import Image\n",
    "\n",
    "# relative keras packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Input, Dropout, Flatten, Convolution2D, MaxPooling2D, Dense, Activation, ZeroPadding2D\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# useful packages from sklearn\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read data from preprocessed pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have preprocessed our data and saved in a pickle file\n",
    "# Now we only need to read this pickle file\n",
    "map_data = pd.read_pickle('position_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RESIZE\n",
    "\n",
    "map_data['image'] = map_data['image'].apply(lambda x: cv2.resize(x,(299,299)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_data.iloc[0,1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column 'index' in the map_data above is used to help match the location data for the 'location' column.\n",
    "\n",
    "We only need the data in the 'label' column and 'image' column to train our model, so we drop those two unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_data.drop(['location','index'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3493, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>[[[68, 43, 39], [75, 51, 47], [84, 60, 57], [9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>[[[67, 38, 37], [69, 37, 36], [70, 38, 38], [7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>[[[65, 51, 59], [70, 55, 78], [86, 75, 84], [9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[[[106, 92, 100], [99, 87, 95], [89, 74, 84], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[[[150, 141, 146], [146, 138, 140], [179, 172,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                              image\n",
       "0     3  [[[68, 43, 39], [75, 51, 47], [84, 60, 57], [9...\n",
       "1     4  [[[67, 38, 37], [69, 37, 36], [70, 38, 38], [7...\n",
       "2     4  [[[65, 51, 59], [70, 55, 78], [86, 75, 84], [9...\n",
       "3     4  [[[106, 92, 100], [99, 87, 95], [89, 74, 84], ...\n",
       "4     4  [[[150, 141, 146], [146, 138, 140], [179, 172,..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the head of our map_data\n",
    "map_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Subset\n",
    "\n",
    "map_data_sub = map_data.iloc[0:600,:]\n",
    "map_data_sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split data for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = pd.DataFrame(map_data_sub.iloc[:,1])\n",
    "ys = pd.DataFrame(map_data_sub.iloc[:,0])\n",
    "ys['label'] = ys['label'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training data (including training and validation) and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into true training data and validation data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the first item of X_train is (299, 299, 3)\n",
      "The length of X_train is 384\n",
      "The shape of the first item of X_val is (299, 299, 3)\n",
      "The length of X_val is 96\n",
      "The shape of the first item of X_test is (299, 299, 3)\n",
      "The length of X_test is 120\n",
      "The type and value of the first item of y_train is <class 'numpy.int64'> 4\n",
      "The length of y_train is 384\n",
      "The shape of the first item of y_val is <class 'numpy.int64'> 3\n",
      "The length of y_val is 96\n",
      "The type and value of the first item of y_test is <class 'numpy.int64'> 4\n",
      "The length of y_test is 120\n"
     ]
    }
   ],
   "source": [
    "# confirm the shape and type of our data is right\n",
    "print('The shape of the first item of X_train is', X_train.iloc[0,0].shape)\n",
    "print('The length of X_train is', len(X_train))\n",
    "\n",
    "print('The shape of the first item of X_val is', X_val.iloc[0,0].shape)\n",
    "print('The length of X_val is', len(X_val))\n",
    "\n",
    "print('The shape of the first item of X_test is', X_test.iloc[0,0].shape)\n",
    "print('The length of X_test is', len(X_test))\n",
    "\n",
    "print('The type and value of the first item of y_train is', type(y_train.iloc[0,0]), y_train.iloc[0,0])\n",
    "print('The length of y_train is', len(y_train))\n",
    "\n",
    "print('The shape of the first item of y_val is', type(y_val.iloc[0,0]), y_val.iloc[0,0])\n",
    "print('The length of y_val is', len(y_val))\n",
    "\n",
    "print('The type and value of the first item of y_test is', type(y_test.iloc[0,0]), y_test.iloc[0,0])\n",
    "print('The length of y_test is', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X data sets equality is: True\n",
      "y data sets equality is: True\n"
     ]
    }
   ],
   "source": [
    "# A small check of total data amount before training the model\n",
    "check_X = (len(X_train) + len(X_val) + len(X_test)) == len(map_data_sub)\n",
    "check_y = (len(y_train) + len(y_val) + len(y_test)) == len(map_data_sub)\n",
    "\n",
    "print('X data sets equality is:', check_X)\n",
    "print('y data sets equality is:', check_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. One-hot encoding and normalizing our input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Create combined lists for training and test sets,\\n# so that we can do normalization and encoding in one function on both data sets.\\n# Note that 'combine' is only a pointer,\\n# so when we change something on data sets in combine,\\n# the original data sets will also change.\\n\\ncombine_X = [X_train, X_val, X_test]\\ncombine_y = [y_train, y_val, y_test]\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Create combined lists for training and test sets,\n",
    "# so that we can do normalization and encoding in one function on both data sets.\n",
    "# Note that 'combine' is only a pointer,\n",
    "# so when we change something on data sets in combine,\n",
    "# the original data sets will also change.\n",
    "\n",
    "combine_X = [X_train, X_val, X_test]\n",
    "combine_y = [y_train, y_val, y_test]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"combine_y = [np.array(y_train['label']), np.array(y_val['label']), np.array(y_test['label'])]\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''combine_y = [np.array(y_train['label']), np.array(y_val['label']), np.array(y_test['label'])]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# normalize inputs from 0-255 to 0.0-1.0\\ncombine_X = combine_X / 255'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# normalize inputs from 0-255 to 0.0-1.0\n",
    "combine_X = combine_X / 255'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# check the value of normalized data\\nprint('The shape and value of X data after normalization:', df.iloc[0,0].shape, '\\n', df.iloc[0,0][0])\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# check the value of normalized data\n",
    "print('The shape and value of X data after normalization:', df.iloc[0,0].shape, '\\n', df.iloc[0,0][0])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize X\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_val = X_val / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### from sklearn.preprocessing imposrt LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#### one hot encoding our categorical data\\n#labelencoder_y = LabelEncoder()\\n#enc = OneHotEncoder()\\nfor df in combine_y:\\n    df = np_utils.to_categorical(df)\\n    #df.iloc[:,0] = labelencoder_y.fit_transform(df.iloc[:,0])\\n    #df = enc.fit(np.array(df).reshape(-1,1))'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#### one hot encoding our categorical data\n",
    "#labelencoder_y = LabelEncoder()\n",
    "#enc = OneHotEncoder()\n",
    "for df in combine_y:\n",
    "    df = np_utils.to_categorical(df)\n",
    "    #df.iloc[:,0] = labelencoder_y.fit_transform(df.iloc[:,0])\n",
    "    #df = enc.fit(np.array(df).reshape(-1,1))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One-hot y\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_val = np_utils.to_categorical(y_val)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Check the type of y after one-hot encoding\\ntype(y_train.iloc[0,0])'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Check the type of y after one-hot encoding\n",
    "type(y_train.iloc[0,0])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is adapted from: \n",
    "http://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 299, 299, 32)      896       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 299, 299, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 299, 299, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 149, 149, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 710432)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               363741696 \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 363,754,405\n",
      "Trainable params: 363,754,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "CNN Model created.\n"
     ]
    }
   ],
   "source": [
    "# First type of CNN model\n",
    "input_size=(299,299,3)\n",
    "num_classes=5\n",
    "\n",
    "def createCNNModel(num_classes):\n",
    "\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 3, 3, input_shape=input_size, border_mode='same', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Convolution2D(32, 3, 3, activation='relu', border_mode='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    epochs = 3  # >>> should be 25+\n",
    "    lrate = 0.01\n",
    "    decay = lrate/epochs\n",
    "    sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model, epochs\n",
    "\n",
    "# create our CNN model\n",
    "model, epochs = createCNNModel(num_classes)\n",
    "print(\"CNN Model created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is adapted from: \n",
    "http://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Another type of CNN model\\ninput_size=(640,640,3)\\nnum_classes=6\\n\\ndef createCNNModel(num_classes):\\n\\n    # Create the model\\n    model = Sequential()\\n    model.add(Convolution2D(32, 3, 3, input_shape=input_size, border_mode=\\'same\\', activation=\\'relu\\'))\\n    model.add(Dropout(0.2))\\n    model.add(MaxPooling2D(pool_size=(2, 2)))\\n    model.add(Flatten())\\n    model.add(Dense(num_classes, activation=\\'softmax\\'))\\n\\n    # Compile model\\n    epochs = 3  # >>> should be 25+\\n    lrate = 0.01\\n    decay = lrate/epochs\\n    sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\\n    model.compile(loss=\\'categorical_crossentropy\\', optimizer=sgd, metrics=[\\'accuracy\\'])\\n    print(model.summary())\\n    return model, epochs\\n\\n# create our CNN model\\nmodel, epochs = createCNNModel(num_classes)\\nprint(\"CNN Model created.\")'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Another type of CNN model\n",
    "input_size=(640,640,3)\n",
    "num_classes=6\n",
    "\n",
    "def createCNNModel(num_classes):\n",
    "\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 3, 3, input_shape=input_size, border_mode='same', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    epochs = 3  # >>> should be 25+\n",
    "    lrate = 0.01\n",
    "    decay = lrate/epochs\n",
    "    sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model, epochs\n",
    "\n",
    "# create our CNN model\n",
    "model, epochs = createCNNModel(num_classes)\n",
    "print(\"CNN Model created.\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = X_train['image'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_array = np.array(X_train_list)\n",
    "y_train_array = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.57254902, 0.59215686, 0.62745098],\n",
       "         [0.54509804, 0.56078431, 0.60784314],\n",
       "         [0.56862745, 0.6       , 0.64313725],\n",
       "         ...,\n",
       "         [0.35294118, 0.23529412, 0.21176471],\n",
       "         [0.34117647, 0.22745098, 0.20784314],\n",
       "         [0.34901961, 0.23137255, 0.21176471]],\n",
       "\n",
       "        [[0.59215686, 0.62352941, 0.6627451 ],\n",
       "         [0.50588235, 0.52941176, 0.60392157],\n",
       "         [0.49803922, 0.51764706, 0.59607843],\n",
       "         ...,\n",
       "         [0.3372549 , 0.22745098, 0.21176471],\n",
       "         [0.35294118, 0.23137255, 0.21176471],\n",
       "         [0.35294118, 0.24313725, 0.22352941]],\n",
       "\n",
       "        [[0.6627451 , 0.69411765, 0.7254902 ],\n",
       "         [0.50980392, 0.52941176, 0.6       ],\n",
       "         [0.50980392, 0.52156863, 0.54117647],\n",
       "         ...,\n",
       "         [0.34509804, 0.23529412, 0.21960784],\n",
       "         [0.34117647, 0.22745098, 0.21176471],\n",
       "         [0.35686275, 0.23137255, 0.20784314]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.35686275, 0.28235294, 0.2627451 ],\n",
       "         [0.77647059, 0.7254902 , 0.71764706],\n",
       "         [0.68627451, 0.61176471, 0.56470588],\n",
       "         ...,\n",
       "         [0.61568627, 0.61176471, 0.61568627],\n",
       "         [0.10588235, 0.08627451, 0.09019608],\n",
       "         [0.10588235, 0.09411765, 0.09019608]],\n",
       "\n",
       "        [[0.43137255, 0.36078431, 0.35294118],\n",
       "         [0.90196078, 0.85490196, 0.85098039],\n",
       "         [0.37647059, 0.31764706, 0.30980392],\n",
       "         ...,\n",
       "         [0.51372549, 0.49411765, 0.49411765],\n",
       "         [0.09803922, 0.08627451, 0.08627451],\n",
       "         [0.13333333, 0.09411765, 0.10588235]],\n",
       "\n",
       "        [[0.8       , 0.73333333, 0.72156863],\n",
       "         [0.57647059, 0.50196078, 0.49019608],\n",
       "         [0.42352941, 0.34901961, 0.3254902 ],\n",
       "         ...,\n",
       "         [0.09411765, 0.09411765, 0.09411765],\n",
       "         [0.11764706, 0.09411765, 0.10196078],\n",
       "         [0.13333333, 0.10196078, 0.10980392]]],\n",
       "\n",
       "\n",
       "       [[[0.4       , 0.28235294, 0.24705882],\n",
       "         [0.49411765, 0.36078431, 0.30980392],\n",
       "         [0.59215686, 0.45490196, 0.39607843],\n",
       "         ...,\n",
       "         [0.41960784, 0.3372549 , 0.34901961],\n",
       "         [0.59215686, 0.50980392, 0.51372549],\n",
       "         [0.76470588, 0.70980392, 0.72941176]],\n",
       "\n",
       "        [[0.3254902 , 0.20784314, 0.18823529],\n",
       "         [0.38823529, 0.26666667, 0.23921569],\n",
       "         [0.43137255, 0.31764706, 0.29019608],\n",
       "         ...,\n",
       "         [0.28235294, 0.17254902, 0.19215686],\n",
       "         [0.33333333, 0.21568627, 0.22352941],\n",
       "         [0.42745098, 0.3254902 , 0.36862745]],\n",
       "\n",
       "        [[0.29411765, 0.16862745, 0.16862745],\n",
       "         [0.32156863, 0.19215686, 0.19215686],\n",
       "         [0.36470588, 0.25490196, 0.23921569],\n",
       "         ...,\n",
       "         [0.38823529, 0.27843137, 0.31764706],\n",
       "         [0.30196078, 0.18039216, 0.22352941],\n",
       "         [0.30980392, 0.18823529, 0.23137255]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.39215686, 0.28235294, 0.2627451 ],\n",
       "         [0.39607843, 0.29019608, 0.26666667],\n",
       "         [0.38039216, 0.26666667, 0.25490196],\n",
       "         ...,\n",
       "         [0.1372549 , 0.13333333, 0.18431373],\n",
       "         [0.50588235, 0.49803922, 0.50980392],\n",
       "         [0.23529412, 0.23529412, 0.23921569]],\n",
       "\n",
       "        [[0.38823529, 0.27843137, 0.24705882],\n",
       "         [0.36862745, 0.25882353, 0.23921569],\n",
       "         [0.36862745, 0.25882353, 0.23921569],\n",
       "         ...,\n",
       "         [0.21176471, 0.22352941, 0.23529412],\n",
       "         [0.24313725, 0.24705882, 0.25098039],\n",
       "         [0.22745098, 0.23137255, 0.23529412]],\n",
       "\n",
       "        [[0.38823529, 0.2745098 , 0.24705882],\n",
       "         [0.37647059, 0.26666667, 0.24313725],\n",
       "         [0.37647059, 0.26666667, 0.24313725],\n",
       "         ...,\n",
       "         [0.23529412, 0.23529412, 0.23921569],\n",
       "         [0.24705882, 0.25098039, 0.25490196],\n",
       "         [0.25098039, 0.25490196, 0.25490196]]],\n",
       "\n",
       "\n",
       "       [[[0.76862745, 0.81176471, 0.85490196],\n",
       "         [0.72156863, 0.74117647, 0.78431373],\n",
       "         [0.72941176, 0.7372549 , 0.78431373],\n",
       "         ...,\n",
       "         [0.75294118, 0.81568627, 0.8745098 ],\n",
       "         [0.3254902 , 0.34509804, 0.41568627],\n",
       "         [0.2627451 , 0.2627451 , 0.34901961]],\n",
       "\n",
       "        [[0.37254902, 0.37647059, 0.42745098],\n",
       "         [0.3254902 , 0.3254902 , 0.37254902],\n",
       "         [0.43529412, 0.43529412, 0.49803922],\n",
       "         ...,\n",
       "         [0.80784314, 0.88627451, 0.93333333],\n",
       "         [0.54509804, 0.57647059, 0.62352941],\n",
       "         [0.17647059, 0.16470588, 0.25490196]],\n",
       "\n",
       "        [[0.30196078, 0.30196078, 0.35294118],\n",
       "         [0.07843137, 0.09411765, 0.17254902],\n",
       "         [0.41568627, 0.43529412, 0.49803922],\n",
       "         ...,\n",
       "         [0.75294118, 0.81568627, 0.87843137],\n",
       "         [0.73333333, 0.75686275, 0.82745098],\n",
       "         [0.08627451, 0.10196078, 0.17647059]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.56862745, 0.60784314, 0.63137255],\n",
       "         [0.57647059, 0.61960784, 0.65490196],\n",
       "         [0.55294118, 0.58431373, 0.61960784],\n",
       "         ...,\n",
       "         [0.15294118, 0.14901961, 0.16078431],\n",
       "         [0.44313725, 0.43921569, 0.45490196],\n",
       "         [0.14117647, 0.13333333, 0.14117647]],\n",
       "\n",
       "        [[0.67058824, 0.69411765, 0.7372549 ],\n",
       "         [0.58823529, 0.61568627, 0.65882353],\n",
       "         [0.58431373, 0.62745098, 0.65098039],\n",
       "         ...,\n",
       "         [0.11372549, 0.10980392, 0.1254902 ],\n",
       "         [0.14117647, 0.13333333, 0.14117647],\n",
       "         [0.12156863, 0.11372549, 0.1254902 ]],\n",
       "\n",
       "        [[0.76470588, 0.8       , 0.83529412],\n",
       "         [0.75294118, 0.78039216, 0.81568627],\n",
       "         [0.66666667, 0.68235294, 0.73333333],\n",
       "         ...,\n",
       "         [0.13333333, 0.1254902 , 0.13333333],\n",
       "         [0.14117647, 0.1372549 , 0.14509804],\n",
       "         [0.12941176, 0.12156863, 0.12941176]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.51372549, 0.45098039, 0.48235294],\n",
       "         [0.76078431, 0.72156863, 0.74117647],\n",
       "         [0.92156863, 0.90196078, 0.90588235],\n",
       "         ...,\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686]],\n",
       "\n",
       "        [[0.27058824, 0.15294118, 0.16470588],\n",
       "         [0.31764706, 0.21960784, 0.23137255],\n",
       "         [0.3372549 , 0.27843137, 0.30196078],\n",
       "         ...,\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686]],\n",
       "\n",
       "        [[0.29803922, 0.16078431, 0.17254902],\n",
       "         [0.24313725, 0.10196078, 0.11372549],\n",
       "         [0.25882353, 0.13333333, 0.1372549 ],\n",
       "         ...,\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.63137255, 0.65490196, 0.70196078],\n",
       "         [0.61176471, 0.65098039, 0.68627451],\n",
       "         [0.60784314, 0.64705882, 0.67843137],\n",
       "         ...,\n",
       "         [0.23137255, 0.23137255, 0.23529412],\n",
       "         [0.51372549, 0.50980392, 0.51372549],\n",
       "         [0.23137255, 0.23137255, 0.23529412]],\n",
       "\n",
       "        [[0.6627451 , 0.64705882, 0.69411765],\n",
       "         [0.65882353, 0.6627451 , 0.70980392],\n",
       "         [0.60784314, 0.61568627, 0.65098039],\n",
       "         ...,\n",
       "         [0.19607843, 0.18823529, 0.2       ],\n",
       "         [0.20392157, 0.19607843, 0.2       ],\n",
       "         [0.21960784, 0.21568627, 0.22352941]],\n",
       "\n",
       "        [[0.59215686, 0.59215686, 0.64313725],\n",
       "         [0.63529412, 0.65490196, 0.70980392],\n",
       "         [0.60784314, 0.65098039, 0.68235294],\n",
       "         ...,\n",
       "         [0.18039216, 0.17647059, 0.18431373],\n",
       "         [0.16470588, 0.16078431, 0.16862745],\n",
       "         [0.21176471, 0.20392157, 0.21176471]]],\n",
       "\n",
       "\n",
       "       [[[0.14509804, 0.21176471, 0.15294118],\n",
       "         [0.14117647, 0.2       , 0.14117647],\n",
       "         [0.27058824, 0.36470588, 0.26666667],\n",
       "         ...,\n",
       "         [0.4627451 , 0.47843137, 0.50588235],\n",
       "         [0.44313725, 0.48235294, 0.49803922],\n",
       "         [0.4627451 , 0.47843137, 0.50196078]],\n",
       "\n",
       "        [[0.17647059, 0.2627451 , 0.18039216],\n",
       "         [0.12941176, 0.22745098, 0.17647059],\n",
       "         [0.24313725, 0.36470588, 0.28235294],\n",
       "         ...,\n",
       "         [0.45098039, 0.46666667, 0.49803922],\n",
       "         [0.47843137, 0.49803922, 0.52156863],\n",
       "         [0.63921569, 0.63529412, 0.63921569]],\n",
       "\n",
       "        [[0.23529412, 0.33333333, 0.25098039],\n",
       "         [0.16862745, 0.2627451 , 0.20392157],\n",
       "         [0.20784314, 0.35294118, 0.27843137],\n",
       "         ...,\n",
       "         [0.47058824, 0.48627451, 0.51764706],\n",
       "         [0.61568627, 0.63137255, 0.64313725],\n",
       "         [0.84705882, 0.85098039, 0.85490196]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.22745098, 0.29019608, 0.26666667],\n",
       "         [0.46666667, 0.45490196, 0.45098039],\n",
       "         [0.27058824, 0.28627451, 0.25882353],\n",
       "         ...,\n",
       "         [0.58431373, 0.58431373, 0.59607843],\n",
       "         [0.09803922, 0.06666667, 0.05098039],\n",
       "         [0.09019608, 0.06666667, 0.05098039]],\n",
       "\n",
       "        [[0.13333333, 0.20784314, 0.16862745],\n",
       "         [0.15686275, 0.19215686, 0.15294118],\n",
       "         [0.15686275, 0.11764706, 0.11372549],\n",
       "         ...,\n",
       "         [0.49019608, 0.49019608, 0.48235294],\n",
       "         [0.10980392, 0.07843137, 0.0627451 ],\n",
       "         [0.0745098 , 0.05882353, 0.04705882]],\n",
       "\n",
       "        [[0.13333333, 0.20784314, 0.16470588],\n",
       "         [0.34901961, 0.4       , 0.37647059],\n",
       "         [0.23921569, 0.24313725, 0.21568627],\n",
       "         ...,\n",
       "         [0.0745098 , 0.06666667, 0.05098039],\n",
       "         [0.09019608, 0.07058824, 0.05098039],\n",
       "         [0.0745098 , 0.05490196, 0.04313725]]],\n",
       "\n",
       "\n",
       "       [[[0.4627451 , 0.37254902, 0.34901961],\n",
       "         [0.4745098 , 0.38039216, 0.35294118],\n",
       "         [0.42745098, 0.32941176, 0.29411765],\n",
       "         ...,\n",
       "         [0.71372549, 0.78823529, 0.85882353],\n",
       "         [0.76470588, 0.84313725, 0.90588235],\n",
       "         [0.75294118, 0.83137255, 0.89803922]],\n",
       "\n",
       "        [[0.43529412, 0.35294118, 0.34901961],\n",
       "         [0.44313725, 0.36078431, 0.34901961],\n",
       "         [0.42352941, 0.32156863, 0.2745098 ],\n",
       "         ...,\n",
       "         [0.6627451 , 0.74117647, 0.81176471],\n",
       "         [0.74117647, 0.81960784, 0.89019608],\n",
       "         [0.80784314, 0.85490196, 0.92941176]],\n",
       "\n",
       "        [[0.42745098, 0.3254902 , 0.29019608],\n",
       "         [0.42745098, 0.3254902 , 0.29019608],\n",
       "         [0.38823529, 0.28627451, 0.24313725],\n",
       "         ...,\n",
       "         [0.70588235, 0.78431373, 0.8627451 ],\n",
       "         [0.72941176, 0.80784314, 0.88235294],\n",
       "         [0.76862745, 0.84313725, 0.90980392]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.48627451, 0.45490196, 0.48627451],\n",
       "         [0.48627451, 0.45490196, 0.48627451],\n",
       "         [0.48627451, 0.45490196, 0.48627451],\n",
       "         ...,\n",
       "         [0.08627451, 0.05098039, 0.04705882],\n",
       "         [0.42745098, 0.40784314, 0.40392157],\n",
       "         [0.08627451, 0.05098039, 0.04705882]],\n",
       "\n",
       "        [[0.49019608, 0.45882353, 0.49019608],\n",
       "         [0.48627451, 0.45490196, 0.48627451],\n",
       "         [0.48627451, 0.45490196, 0.48627451],\n",
       "         ...,\n",
       "         [0.08627451, 0.05098039, 0.04705882],\n",
       "         [0.08627451, 0.05098039, 0.04705882],\n",
       "         [0.08627451, 0.05098039, 0.04705882]],\n",
       "\n",
       "        [[0.49803922, 0.47058824, 0.49803922],\n",
       "         [0.49411765, 0.4627451 , 0.49411765],\n",
       "         [0.48627451, 0.45490196, 0.48627451],\n",
       "         ...,\n",
       "         [0.08627451, 0.05098039, 0.04705882],\n",
       "         [0.08627451, 0.05098039, 0.04705882],\n",
       "         [0.08627451, 0.05098039, 0.04705882]]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "384/384 [==============================] - 194s 506ms/step - loss: 3.8139 - acc: 0.2734\n",
      "Epoch 2/3\n",
      "384/384 [==============================] - 166s 431ms/step - loss: 1.6861 - acc: 0.3724\n",
      "Epoch 3/3\n",
      "384/384 [==============================] - 155s 403ms/step - loss: 1.5740 - acc: 0.3490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20880284470>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch size，learning rate can be modified before training\n",
    "\n",
    "batch_size=60\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "model.fit(X_train_array, y_train_array, batch_size = batch_size, nb_epoch = epochs)\n",
    "#model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=epochs, batch_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_array = np.array(X_test['image'].tolist())\n",
    "y_test_array = np.array(y_test)\n",
    "X_val_array =np.array(X_val['image'].tolist())\n",
    "y_val_array = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 39.84%\n",
      "val Accuracy: 30.21%\n",
      "Test Accuracy: 40.00%\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_train_array, y_train_array, verbose=0)\n",
    "print(\"Train Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "scores = model.evaluate(X_val_array, y_val_array, verbose=0)\n",
    "print(\"val Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test_array, y_test_array, verbose=0)\n",
    "print(\"Test Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(X_test)\n",
    "print(\"y_pred is \",y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##store the weights\n",
    "for layer in model.layers:\n",
    "    weights = layer.get_weights() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## store the model\n",
    "model.save('cnn_trial.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###read the stored the model\n",
    "import h5py\n",
    "\n",
    "def print_keras_wegiths(weight_file_path):\n",
    "    f = h5py.File(weight_file_path)  # 读取weights h5文件返回File类\n",
    "    try:\n",
    "        if len(f.attrs.items()):\n",
    "            print(\"{} contains: \".format(weight_file_path))\n",
    "            print(\"Root attributes:\")\n",
    "        for key, value in f.attrs.items():\n",
    "            print(\"  {}: {}\".format(key, value))  # 输出储存在File类中的attrs信息，一般是各层的名称\n",
    "\n",
    "        for layer, g in f.items():  # 读取各层的名称以及包含层信息的Group类\n",
    "            print(\"  {}\".format(layer))\n",
    "            print(\"    Attributes:\")\n",
    "            for key, value in g.attrs.items(): # 输出储存在Group类中的attrs信息，一般是各层的weights和bias及他们的名称\n",
    "                print(\"      {}: {}\".format(key, value))  \n",
    "\n",
    "            print(\"    Dataset:\")\n",
    "            for name, d in g.items(): # 读取各层储存具体信息的Dataset类\n",
    "                print(\"      {}: {}\".format(name, d.value.shape)) # 输出储存在Dataset中的层名称和权重，也可以打印dataset的attrs，但是keras中是空的\n",
    "                print(\"      {}: {}\".format(name. d.value))\n",
    "    finally:\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
