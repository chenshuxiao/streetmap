{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Package Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The backend is: tensorflow\n",
      "tf\n",
      "1.9.0\n",
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "\n",
    "# import keras\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf') # note that we need to have tensorflow dimension ordering still because of the weigths.\n",
    "print('The backend is:',K.backend())\n",
    "import tensorflow as tf\n",
    "print(K.image_dim_ordering()) # should say tf\n",
    "print(tf.__version__) # tested for 1.11.0\n",
    "import cv2\n",
    "import keras\n",
    "print(keras.__version__) # tested for 2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "from __future__ import absolute_import, division, print_function # make it compatible w Python 2\n",
    "import os\n",
    "import h5py # to handle weights\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# to read image\n",
    "from PIL import Image\n",
    "\n",
    "# relative keras packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Input, Dropout, Flatten, Convolution2D, MaxPooling2D, Dense, Activation, ZeroPadding2D\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# useful packages from sklearn\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read data from preprocessed pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have preprocessed our data and saved in a pickle file\n",
    "# Now we only need to read this pickle file\n",
    "map_data = pd.read_pickle('position_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column 'index' in the map_data above is used to help match the location data for the 'location' column.\n",
    "\n",
    "We only need the data in the 'label' column and 'image' column to train our model, so we drop those two unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize from 640 * 640 * 3\n",
    "\n",
    "map_data['image'] = map_data['image'].apply(lambda x: cv2.resize(x,(200,125)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_data.drop(['location','index'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>[[[85, 59, 55], [72, 44, 40], [84, 57, 53], [9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>[[[66, 42, 42], [70, 37, 37], [72, 40, 40], [6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>[[[66, 52, 79], [78, 62, 77], [81, 68, 80], [8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[[[111, 96, 104], [100, 85, 92], [159, 151, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[[[202, 195, 196], [203, 198, 198], [95, 83, 9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                              image\n",
       "0     3  [[[85, 59, 55], [72, 44, 40], [84, 57, 53], [9...\n",
       "1     4  [[[66, 42, 42], [70, 37, 37], [72, 40, 40], [6...\n",
       "2     4  [[[66, 52, 79], [78, 62, 77], [81, 68, 80], [8...\n",
       "3     4  [[[111, 96, 104], [100, 85, 92], [159, 151, 15...\n",
       "4     4  [[[202, 195, 196], [203, 198, 198], [95, 83, 9..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the head of our map_data\n",
    "map_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split data for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read X and y data\n",
    "\n",
    "Xs = np.array(map_data.iloc[:,1].tolist())\n",
    "ys = np.array(map_data.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training data (including training and validation) and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Split training data into true training data and validation data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of X_train is 2235\n",
      "The length of X_val is 559\n",
      "The length of X_test is 699\n",
      "The length of y_train is 2235\n",
      "The length of y_val is 559\n",
      "The length of y_test is 699\n"
     ]
    }
   ],
   "source": [
    "# confirm the shape and type of our data is right\n",
    "print('The length of X_train is', len(X_train))\n",
    "\n",
    "print('The length of X_val is', len(X_val))\n",
    "\n",
    "print('The length of X_test is', len(X_test))\n",
    "\n",
    "print('The length of y_train is', len(y_train))\n",
    "\n",
    "print('The length of y_val is', len(y_val))\n",
    "\n",
    "print('The length of y_test is', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X data sets equality is: True\n",
      "y data sets equality is: True\n"
     ]
    }
   ],
   "source": [
    "# A small check of total data amount before training the model\n",
    "check_X = (len(X_train) + len(X_val) + len(X_test)) == len(map_data)\n",
    "check_y = (len(y_train) + len(y_val) + len(y_test)) == len(map_data)\n",
    "\n",
    "print('X data sets equality is:', check_X)\n",
    "print('y data sets equality is:', check_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. One-hot encoding and normalizing our input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0.0-1.0\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_val = np_utils.to_categorical(y_val)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is adapted from: \n",
    "http://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 125, 200, 32)      896       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 125, 200, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 125, 200, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 62, 100, 32)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 198400)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               101581312 \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 101,594,021\n",
      "Trainable params: 101,594,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "CNN Model created.\n"
     ]
    }
   ],
   "source": [
    "# First type of CNN model\n",
    "input_size=(125,200,3)\n",
    "num_classes=5\n",
    "\n",
    "def createCNNModel(num_classes):\n",
    "\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 3, 3, input_shape=input_size, border_mode='same', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Convolution2D(32, 3, 3, activation='relu', border_mode='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    epochs = 3  # >>> should be 25+\n",
    "    lrate = 0.01\n",
    "    decay = lrate/epochs\n",
    "    sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model, epochs\n",
    "\n",
    "# create our CNN model\n",
    "model, epochs = createCNNModel(num_classes)\n",
    "print(\"CNN Model created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is adapted from: \n",
    "http://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 125, 200, 32)      896       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 125, 200, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 62, 100, 32)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 198400)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 992005    \n",
      "=================================================================\n",
      "Total params: 992,901\n",
      "Trainable params: 992,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "CNN Model created.\n"
     ]
    }
   ],
   "source": [
    "# Another type of CNN model\n",
    "input_size=(125,200,3)\n",
    "num_classes=5\n",
    "\n",
    "def createCNNModel(num_classes):\n",
    "\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 3, 3, input_shape=input_size, border_mode='same', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    epochs = 3  # >>> should be 25+\n",
    "    lrate = 0.01\n",
    "    decay = lrate/epochs\n",
    "    sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model, epochs\n",
    "\n",
    "# create our CNN model\n",
    "model, epochs = createCNNModel(num_classes)\n",
    "print(\"CNN Model created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2235, 125, 200, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2235/2235 [==============================] - 2353s 1s/step - loss: 1.6213 - acc: 0.2653\n",
      "Epoch 2/3\n",
      "2235/2235 [==============================] - 1815s 812ms/step - loss: 1.4533 - acc: 0.3588\n",
      "Epoch 3/3\n",
      "2235/2235 [==============================] - 2052s 918ms/step - loss: 1.3322 - acc: 0.4157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b2ee0b518>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch sizeï¼Œlearning rate can be modified before training\n",
    "\n",
    "batch_size=60\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "model.fit(X_train, y_train, batch_size = batch_size, nb_epoch = epochs)\n",
    "#model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=epochs, batch_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 42.78%\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model on test\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 52.84%\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model on test\n",
    "scores = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 43.47%\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model on test\n",
    "scores = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(\"Validation Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred is  [[0.16616884 0.1834371  0.28311354 0.24392208 0.12335841]\n",
      " [0.36161795 0.20711294 0.23451313 0.1337956  0.0629604 ]\n",
      " [0.282091   0.33298552 0.14026824 0.10466771 0.13998757]\n",
      " ...\n",
      " [0.594185   0.2859091  0.0856473  0.02471039 0.00954829]\n",
      " [0.12159665 0.09033503 0.42159232 0.30151638 0.06495959]\n",
      " [0.45229322 0.3620299  0.08876586 0.05765385 0.03925715]]\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)\n",
    "print(\"y_pred is \",y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##store the weights\n",
    "for layer in model.layers:\n",
    "    weights = layer.get_weights() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.08077971, -0.08494316,  0.10738301, -0.02133705,  0.00033168],\n",
       "        [-0.03683984, -0.08718871, -0.08432052,  0.08939377, -0.01828957],\n",
       "        [-0.01485827,  0.01543096,  0.05174096,  0.08669905,  0.01566698],\n",
       "        ...,\n",
       "        [ 0.00121219, -0.05390524, -0.00892309, -0.0747997 , -0.03409497],\n",
       "        [-0.0796201 , -0.08361286, -0.07777672, -0.07893319, -0.04651123],\n",
       "        [ 0.01004251,  0.05703814,  0.00833634, -0.00343164, -0.03502527]],\n",
       "       dtype=float32),\n",
       " array([-0.02979133,  0.01614214,  0.0238754 , -0.00688321, -0.00334301],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## store the model\n",
    "model.save('cnn_trial_with_2235.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###read the stored the model\n",
    "import h5py\n",
    "\n",
    "def print_keras_wegiths(weight_file_path):\n",
    "    # Read weights_h5 file and give a 'file'\n",
    "    \n",
    "    f = h5py.File(weight_file_path)  \n",
    "    \n",
    "    try:\n",
    "        if len(f.attrs.items()):\n",
    "            print(\"{} contains: \".format(weight_file_path))\n",
    "            print(\"Root attributes:\")\n",
    "        for key, value in f.attrs.items():\n",
    "            print(\"  {}: {}\".format(key, value))  \n",
    "            # Output: attrs information stored in the file\n",
    "            # Generally speaking, it's the names of each layer\n",
    "\n",
    "        for layer, g in f.items():  \n",
    "            # Read names of each layer and the arrs group containing information of each layer\n",
    "            print(\"  {}\".format(layer))\n",
    "            print(\"    Attributes:\")\n",
    "            for key, value in g.attrs.items(): \n",
    "                print(\"      {}: {}\".format(key, value))  \n",
    "\n",
    "            print(\"    Dataset:\")\n",
    "            for name, d in g.items():\n",
    "                print(\"      {}: {}\".format(name, d.value.shape))\n",
    "                print(\"      {}: {}\".format(name. d.value))\n",
    "    finally:\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
