{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The backend is: tensorflow\n",
      "tf\n",
      "1.12.0\n",
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf') # note that we need to have tensorflow dimension ordering still because of the weigths.\n",
    "print('The backend is:',K.backend())\n",
    "import tensorflow as tf\n",
    "print(K.image_dim_ordering()) # should say tf\n",
    "print(tf.__version__) # tested for 1.11.0\n",
    "import cv2\n",
    "import keras\n",
    "print(keras.__version__) # tested for 2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "from __future__ import absolute_import, division, print_function # make it compatible w Python 2\n",
    "import os\n",
    "import h5py # to handle weights\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Input, Dropout, Flatten, Convolution2D, MaxPooling2D, Dense, Activation, ZeroPadding2D\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># URLs pointing our resources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://data.cityofnewyork.us/Public-Safety/NY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       # URLs pointing our resources\n",
       "0  https://data.cityofnewyork.us/Public-Safety/NY..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/Users/xudanluo/Desktop/jupyter notebook/290/AI FOR GOOD/streetmap-master/resources.dat\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testimage_big.png\n",
      "4.png\n",
      "2.png\n",
      "3.png\n",
      "1.png\n",
      "xxxx.png\n",
      "x.png\n",
      "xxx.png\n",
      "xx.png\n",
      "samples= 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(640, 640, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import maps, note all map images are unique\n",
    "imgs=[]\n",
    "for path, dirs, files in os.walk('/Users/xudanluo/Desktop/未命名文件夹'):#file path ending with folder\n",
    "    #print('FOLDER',path)\n",
    "    #print(\"\\n\",files[1:-1])\n",
    "    for f in files[1:]:\n",
    "        imgs.append(cv2.imread(path+\"/\"+f))\n",
    "        print(f)\n",
    "        #imgspath.append(path+\"/\"+f)\n",
    "print(\"samples=\",len(imgs))\n",
    "imgs[-1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inpput labels\n",
    "labels=[0,1,5,3,4,5,2,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5]\n"
     ]
    }
   ],
   "source": [
    "#split the data\n",
    "def cross_validate(Xs, ys):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.2, random_state=0)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validate(imgs, labels)\n",
    "\n",
    "# confirm we got our data\n",
    "print(y_test[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 and 0.0-1.0\n",
    "X_train = np.array(X_train).astype('float32')\n",
    "X_test = np.array(X_test).astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "#num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 2, 2, 3, 0, 5])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##潘潘你可以直接改这个或者直接run这个model\n",
    "input_size=(640,640,3)\n",
    "num_classes=6\n",
    "\n",
    "def createCNNModel(num_classes):\n",
    "#    \"\"\" Adapted from: # http://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/\n",
    "# \"\"\"\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 3, 3, input_shape=input_size, border_mode='same', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Convolution2D(32, 3, 3, activation='relu', border_mode='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    epochs = 3  # >>> should be 25+\n",
    "    lrate = 0.01\n",
    "    decay = lrate/epochs\n",
    "    sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model, epochs\n",
    "\n",
    "# create our CNN model\n",
    "model, epochs = createCNNModel(num_classes)\n",
    "print(\"CNN Model created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 640, 640, 32)      896       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 640, 640, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 320, 320, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3276800)           0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 19660806  \n",
      "=================================================================\n",
      "Total params: 19,661,702\n",
      "Trainable params: 19,661,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "CNN Model created.\n"
     ]
    }
   ],
   "source": [
    "#这个模块我今天run，你不用run他\n",
    "input_size=(640,640,3)\n",
    "num_classes=6\n",
    "def createCNNModel(num_classes):\n",
    "#    \"\"\" Adapted from: # http://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/\n",
    "# \"\"\"\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 3, 3, input_shape=input_size, border_mode='same', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    epochs = 3  # >>> should be 25+\n",
    "    lrate = 0.01\n",
    "    decay = lrate/epochs\n",
    "    sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model, epochs\n",
    "\n",
    "# create our CNN model\n",
    "model, epochs = createCNNModel(num_classes)\n",
    "print(\"CNN Model created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 640, 640, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7/7 [==============================] - 11s 2s/step - loss: 1.9487 - acc: 0.1429\n",
      "Epoch 2/3\n",
      "7/7 [==============================] - 7s 1s/step - loss: 11.5129 - acc: 0.2857\n",
      "Epoch 3/3\n",
      "7/7 [==============================] - 7s 1s/step - loss: 11.5129 - acc: 0.2857\n",
      "Accuracy: 0.00%\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#batch size，learning rate之类的这些 你跑之前都可以改\n",
    "batch_size=60\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "model.fit(X_train, y_train, batch_size = batch_size, nb_epoch = epochs)\n",
    "#model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=epochs, batch_size=60)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)\n",
    "print(\"y_pred is \",y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "##store the weights\n",
    "for layer in model.layers:\n",
    "    weights = layer.get_weights() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.6228707e-03, -3.4538349e-03,  3.2287699e-03,  1.5444639e-03,\n",
       "          3.3674845e-03, -5.2891597e-03],\n",
       "        [-7.8277901e-04, -6.5064844e-04,  6.8643334e-04, -7.4463169e-04,\n",
       "          6.5404869e-04, -1.8925820e-03],\n",
       "        [-4.0262926e-04,  1.1396382e-03, -4.4183107e-06,  8.5334317e-04,\n",
       "         -7.6675159e-04,  9.5519121e-04],\n",
       "        ...,\n",
       "        [-1.0649031e-03,  7.1342685e-04,  7.4985437e-04,  1.2977265e-03,\n",
       "          6.8004243e-04, -7.2314538e-04],\n",
       "        [ 1.0233629e-03, -4.4777343e-04,  5.0101511e-04,  3.5858125e-04,\n",
       "         -9.0478320e-04, -1.4484559e-04],\n",
       "        [ 8.5067679e-04, -6.8056706e-04,  1.0558066e-03,  1.2486810e-03,\n",
       "          2.8237549e-04,  1.2023586e-03]], dtype=float32),\n",
       " array([ 0.00166985, -0.00310872,  0.00293007,  0.00101551,  0.00228385,\n",
       "        -0.00479055], dtype=float32)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "##store the model\n",
    "model.save('/Users/xudanluo/Desktop/未命名文件夹/my_model.h5')#change path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###read the stored the model\n",
    "import h5py\n",
    "\n",
    "def print_keras_wegiths(weight_file_path):\n",
    "    f = h5py.File(weight_file_path)  # 读取weights h5文件返回File类\n",
    "    try:\n",
    "        if len(f.attrs.items()):\n",
    "            print(\"{} contains: \".format(weight_file_path))\n",
    "            print(\"Root attributes:\")\n",
    "        for key, value in f.attrs.items():\n",
    "            print(\"  {}: {}\".format(key, value))  # 输出储存在File类中的attrs信息，一般是各层的名称\n",
    "\n",
    "        for layer, g in f.items():  # 读取各层的名称以及包含层信息的Group类\n",
    "            print(\"  {}\".format(layer))\n",
    "            print(\"    Attributes:\")\n",
    "            for key, value in g.attrs.items(): # 输出储存在Group类中的attrs信息，一般是各层的weights和bias及他们的名称\n",
    "                print(\"      {}: {}\".format(key, value))  \n",
    "\n",
    "            print(\"    Dataset:\")\n",
    "            for name, d in g.items(): # 读取各层储存具体信息的Dataset类\n",
    "                print(\"      {}: {}\".format(name, d.value.shape)) # 输出储存在Dataset中的层名称和权重，也可以打印dataset的attrs，但是keras中是空的\n",
    "                print(\"      {}: {}\".format(name. d.value))\n",
    "    finally:\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
